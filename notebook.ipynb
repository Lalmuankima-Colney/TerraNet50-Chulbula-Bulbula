{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":127593,"databundleVersionId":15603876,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q segmentation-models\n!pip install -q albumentations\nimport os\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nimport segmentation_models as sm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-08T12:50:31.466759Z","iopub.execute_input":"2026-02-08T12:50:31.466954Z","iopub.status.idle":"2026-02-08T12:50:54.041540Z","shell.execute_reply.started":"2026-02-08T12:50:31.466933Z","shell.execute_reply":"2026-02-08T12:50:54.040726Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"2026-02-08 12:50:41.518868: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770555041.696137      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770555041.750561      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770555042.204756      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770555042.204790      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770555042.204793      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770555042.204795      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Segmentation Models: using `tf.keras` framework.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport os\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nimport segmentation_models as sm\n\nimport cv2\nimport glob\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import layers, models, callbacks, backend as K\n\n# --- 1. CONFIGURATION ---\nDATASET_ROOT = \"/kaggle/input/terra-seg-rugged-terrain-segmentation/offroad-seg-kaggle\"\nTRAIN_IMG_DIR = os.path.join(DATASET_ROOT, \"train_images\")\nTRAIN_MASK_DIR = os.path.join(DATASET_ROOT, \"train_masks\")\nTEST_IMG_DIR = os.path.join(DATASET_ROOT, \"test_images_padded\")\n\n# Correct Resolution & Backbone\nIMG_H = 544 \nIMG_W = 960 \nBATCH_SIZE = 4 \nEPOCHS = 7    \nLR = 1e-4\nN_FOLDS = 5     \n\nBACKBONE = 'seresnext50' \npreprocess_input = sm.get_preprocessing(BACKBONE)\n\n# --- 2. AUGMENTATION (Optimized for Terrain) ---\ntrain_transform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    # Vertical Flip removed (Terrain is never upside down)\n    A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=15, shift_limit=0.1, p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n    A.RGBShift(p=0.3), # Crucial for lighting changes\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n])\n\n# --- 3. DATA LOADER ---\ndef process_path(file_name, augment=False):\n    file_name = file_name.decode()\n    img_path = os.path.join(TRAIN_IMG_DIR, file_name)\n    mask_path = os.path.join(TRAIN_MASK_DIR, file_name)\n    \n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    \n    if augment:\n        aug = train_transform(image=img, mask=mask)\n        img, mask = aug['image'], aug['mask']\n        \n    img = cv2.resize(img, (IMG_W, IMG_H))\n    mask = cv2.resize(mask, (IMG_W, IMG_H), interpolation=cv2.INTER_NEAREST)\n    \n    img = preprocess_input(img).astype(np.float32)\n    mask = np.isin(mask, [27, 39]).astype(np.float32)\n    \n    return img, mask[..., np.newaxis]\n\ndef tf_process_dataset(file_paths, is_train=False):\n    ds = tf.data.Dataset.from_tensor_slices(file_paths)\n    if is_train:\n        ds = ds.shuffle(len(file_paths))\n        ds = ds.map(lambda x: tf.numpy_function(process_path, [x, True], [tf.float32, tf.float32]), num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        ds = ds.map(lambda x: tf.numpy_function(process_path, [x, False], [tf.float32, tf.float32]), num_parallel_calls=tf.data.AUTOTUNE)\n    \n    ds = ds.map(lambda x, y: (tf.ensure_shape(x, [IMG_H, IMG_W, 3]), tf.ensure_shape(y, [IMG_H, IMG_W, 1])))\n    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n# --- 4. MODEL & LOSS ---\ndef bce_dice_loss(y_true, y_pred):\n    bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n    dice = sm.losses.DiceLoss()(y_true, y_pred)\n    return bce + dice\n\ndef build_model():\n    model = sm.Unet(\n        BACKBONE, \n        classes=1, \n        activation='sigmoid', \n        encoder_weights='imagenet',\n        decoder_block_type='transpose'\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(LR),\n        loss=bce_dice_loss,\n        metrics=[sm.metrics.IOUScore(threshold=0.5)]\n    )\n    return model\n\n# --- 5. TRAINING LOOP (K-FOLD) ---\nall_files = np.array(sorted(os.listdir(TRAIN_IMG_DIR)))\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\nbest_thresholds = []\n\nprint(f\"ğŸš€ Starting {N_FOLDS}-Fold Cross-Validation with {BACKBONE}...\")\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(all_files)):\n    print(f\"\\n--- FOLD {fold+1}/{N_FOLDS} ---\")\n    \n    train_ds = tf_process_dataset(all_files[train_idx], is_train=True)\n    val_ds = tf_process_dataset(all_files[val_idx], is_train=False)\n    \n    K.clear_session()\n    model = build_model()\n    \n    checkpoint_name = f'model_fold_{fold+1}.keras'\n    checkpoint = callbacks.ModelCheckpoint(checkpoint_name, save_best_only=True, monitor='val_iou_score', mode='max', verbose=0)\n    lr_schedule = callbacks.LearningRateScheduler(lambda epoch: LR * 0.5 * (1 + np.cos(np.pi * epoch / EPOCHS)))\n    \n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS,\n        callbacks=[checkpoint, lr_schedule],\n        verbose=1\n    )\n    \n    # Validation Threshold Tuning\n    model.load_weights(checkpoint_name)\n    val_preds = model.predict(val_ds, verbose=0)\n    val_masks = np.vstack([mask.numpy() for _, mask in val_ds])\n    \n    best_iou, best_th = 0, 0.5\n    for th in np.arange(0.3, 0.7, 0.05):\n        score = sm.metrics.IOUScore(threshold=th)(val_masks, val_preds).numpy()\n        if score > best_iou: best_iou, best_th = score, th\n            \n    best_thresholds.append(best_th)\n    print(f\"âœ… Fold {fold+1} Best Threshold: {best_th:.2f} | IoU: {best_iou:.4f}\")\n    \n    del model, train_ds, val_ds, val_preds, val_masks\n    gc.collect()\n\navg_threshold = np.mean(best_thresholds)\nprint(f\"\\nğŸ† Average Best Threshold: {avg_threshold:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T12:59:20.429501Z","iopub.execute_input":"2026-02-08T12:59:20.429833Z","iopub.status.idle":"2026-02-08T17:00:42.849985Z","shell.execute_reply.started":"2026-02-08T12:59:20.429804Z","shell.execute_reply":"2026-02-08T17:00:42.846950Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/3610870800.py:40: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n  A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Starting 5-Fold Cross-Validation with seresnext50...\n\n--- FOLD 1/5 ---\nEpoch 1/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 749ms/step - iou_score: 0.7665 - loss: 0.5916 - val_iou_score: 0.8559 - val_loss: 0.3242 - learning_rate: 1.0000e-04\nEpoch 2/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 538ms/step - iou_score: 0.8513 - loss: 0.3270 - val_iou_score: 0.8655 - val_loss: 0.2991 - learning_rate: 9.5048e-05\nEpoch 3/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 538ms/step - iou_score: 0.8625 - loss: 0.2956 - val_iou_score: 0.8709 - val_loss: 0.2810 - learning_rate: 8.1174e-05\nEpoch 4/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 539ms/step - iou_score: 0.8688 - loss: 0.2806 - val_iou_score: 0.8748 - val_loss: 0.2713 - learning_rate: 6.1126e-05\nEpoch 5/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 538ms/step - iou_score: 0.8715 - loss: 0.2722 - val_iou_score: 0.8778 - val_loss: 0.2662 - learning_rate: 3.8874e-05\nEpoch 6/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 537ms/step - iou_score: 0.8729 - loss: 0.2679 - val_iou_score: 0.8785 - val_loss: 0.2636 - learning_rate: 1.8826e-05\nEpoch 7/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 538ms/step - iou_score: 0.8725 - loss: 0.2700 - val_iou_score: 0.8789 - val_loss: 0.2627 - learning_rate: 4.9516e-06\nâœ… Fold 1 Best Threshold: 0.50 | IoU: 0.8848\n\n--- FOLD 2/5 ---\nEpoch 1/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 742ms/step - iou_score: 0.7487 - loss: 0.5716 - val_iou_score: 0.8558 - val_loss: 0.3192 - learning_rate: 1.0000e-04\nEpoch 2/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 535ms/step - iou_score: 0.8532 - loss: 0.3156 - val_iou_score: 0.8653 - val_loss: 0.2963 - learning_rate: 9.5048e-05\nEpoch 3/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 539ms/step - iou_score: 0.8654 - loss: 0.2877 - val_iou_score: 0.8696 - val_loss: 0.2859 - learning_rate: 8.1174e-05\nEpoch 4/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 540ms/step - iou_score: 0.8668 - loss: 0.2835 - val_iou_score: 0.8740 - val_loss: 0.2755 - learning_rate: 6.1126e-05\nEpoch 5/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 540ms/step - iou_score: 0.8738 - loss: 0.2668 - val_iou_score: 0.8757 - val_loss: 0.2697 - learning_rate: 3.8874e-05\nEpoch 6/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 538ms/step - iou_score: 0.8743 - loss: 0.2659 - val_iou_score: 0.8761 - val_loss: 0.2678 - learning_rate: 1.8826e-05\nEpoch 7/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 537ms/step - iou_score: 0.8757 - loss: 0.2624 - val_iou_score: 0.8770 - val_loss: 0.2665 - learning_rate: 4.9516e-06\nâœ… Fold 2 Best Threshold: 0.50 | IoU: 0.8827\n\n--- FOLD 3/5 ---\nEpoch 1/7\n","output_type":"stream"},{"name":"stderr","text":"2026-02-08 14:40:39.791571: E external/local_xla/xla/service/slow_operation_alarm.cc:73] \n********************************\n[Compiling module a_inference_one_step_on_data_883320__.89303] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n********************************\n2026-02-08 14:40:53.721590: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2m13.930150911s\n\n********************************\n[Compiling module a_inference_one_step_on_data_883320__.89303] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n********************************\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 738ms/step - iou_score: 0.7684 - loss: 0.5545 - val_iou_score: 0.8534 - val_loss: 0.3286 - learning_rate: 1.0000e-04\nEpoch 2/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 529ms/step - iou_score: 0.8539 - loss: 0.3168 - val_iou_score: 0.8675 - val_loss: 0.2945 - learning_rate: 9.5048e-05\nEpoch 3/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 530ms/step - iou_score: 0.8609 - loss: 0.2958 - val_iou_score: 0.8694 - val_loss: 0.2833 - learning_rate: 8.1174e-05\nEpoch 4/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 530ms/step - iou_score: 0.8690 - loss: 0.2792 - val_iou_score: 0.8747 - val_loss: 0.2716 - learning_rate: 6.1126e-05\nEpoch 5/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 529ms/step - iou_score: 0.8712 - loss: 0.2736 - val_iou_score: 0.8782 - val_loss: 0.2655 - learning_rate: 3.8874e-05\nEpoch 6/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 529ms/step - iou_score: 0.8723 - loss: 0.2699 - val_iou_score: 0.8789 - val_loss: 0.2622 - learning_rate: 1.8826e-05\nEpoch 7/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 527ms/step - iou_score: 0.8726 - loss: 0.2681 - val_iou_score: 0.8792 - val_loss: 0.2615 - learning_rate: 4.9516e-06\nâœ… Fold 3 Best Threshold: 0.45 | IoU: 0.8852\n\n--- FOLD 4/5 ---\nEpoch 1/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m759s\u001b[0m 763ms/step - iou_score: 0.7572 - loss: 0.5910 - val_iou_score: 0.8530 - val_loss: 0.3348 - learning_rate: 1.0000e-04\nEpoch 2/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 541ms/step - iou_score: 0.8536 - loss: 0.3181 - val_iou_score: 0.8629 - val_loss: 0.3004 - learning_rate: 9.5048e-05\nEpoch 3/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 534ms/step - iou_score: 0.8642 - loss: 0.2896 - val_iou_score: 0.8642 - val_loss: 0.2981 - learning_rate: 8.1174e-05\nEpoch 4/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 535ms/step - iou_score: 0.8672 - loss: 0.2824 - val_iou_score: 0.8706 - val_loss: 0.2792 - learning_rate: 6.1126e-05\nEpoch 5/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 539ms/step - iou_score: 0.8697 - loss: 0.2758 - val_iou_score: 0.8725 - val_loss: 0.2766 - learning_rate: 3.8874e-05\nEpoch 6/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 537ms/step - iou_score: 0.8728 - loss: 0.2683 - val_iou_score: 0.8731 - val_loss: 0.2725 - learning_rate: 1.8826e-05\nEpoch 7/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 536ms/step - iou_score: 0.8745 - loss: 0.2656 - val_iou_score: 0.8740 - val_loss: 0.2714 - learning_rate: 4.9516e-06\nâœ… Fold 4 Best Threshold: 0.50 | IoU: 0.8797\n\n--- FOLD 5/5 ---\nEpoch 1/7\n","output_type":"stream"},{"name":"stderr","text":"2026-02-08 16:18:14.718566: E external/local_xla/xla/service/slow_operation_alarm.cc:73] \n********************************\n[Compiling module a_inference_one_step_on_data_1461080__.89303] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n********************************\n2026-02-08 16:18:35.593424: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2m20.875045676s\n\n********************************\n[Compiling module a_inference_one_step_on_data_1461080__.89303] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n********************************\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 568ms/step - iou_score: 0.7572 - loss: 0.5823 - val_iou_score: 0.8580 - val_loss: 0.3206 - learning_rate: 1.0000e-04\nEpoch 2/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 535ms/step - iou_score: 0.8539 - loss: 0.3176 - val_iou_score: 0.8554 - val_loss: 0.3210 - learning_rate: 9.5048e-05\nEpoch 3/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 541ms/step - iou_score: 0.8653 - loss: 0.2893 - val_iou_score: 0.8650 - val_loss: 0.2972 - learning_rate: 8.1174e-05\nEpoch 4/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 540ms/step - iou_score: 0.8690 - loss: 0.2782 - val_iou_score: 0.8751 - val_loss: 0.2746 - learning_rate: 6.1126e-05\nEpoch 5/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 538ms/step - iou_score: 0.8749 - loss: 0.2661 - val_iou_score: 0.8752 - val_loss: 0.2714 - learning_rate: 3.8874e-05\nEpoch 6/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 537ms/step - iou_score: 0.8737 - loss: 0.2660 - val_iou_score: 0.8782 - val_loss: 0.2648 - learning_rate: 1.8826e-05\nEpoch 7/7\n\u001b[1m635/635\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 535ms/step - iou_score: 0.8755 - loss: 0.2637 - val_iou_score: 0.8783 - val_loss: 0.2642 - learning_rate: 4.9516e-06\nâœ… Fold 5 Best Threshold: 0.45 | IoU: 0.8840\n\nğŸ† Average Best Threshold: 0.480\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport segmentation_models as sm\nfrom tqdm.notebook import tqdm\n\n# --- 1. CONFIGURATION ---\nDATASET_ROOT = \"/kaggle/input/terra-seg-rugged-terrain-segmentation/offroad-seg-kaggle\"\nTEST_IMG_DIR = os.path.join(DATASET_ROOT, \"test_images_padded\")\n\n# ğŸŸ¢ CHANGE YOUR THRESHOLD HERE ğŸŸ¢\nNEW_THRESHOLD = 0.5  # <--- Edit this value (e.g. 0.3, 0.45, 0.6)\n\n# Model Settings (Must match training)\nIMG_H = 544 \nIMG_W = 960 \nBACKBONE = 'seresnext50'\npreprocess_input = sm.get_preprocessing(BACKBONE)\nN_FOLDS = 5\n\n# --- 2. HELPER FUNCTIONS ---\ndef build_model():\n    # Re-construct architecture to load weights into\n    return sm.Unet(\n        BACKBONE, \n        classes=1, \n        activation='sigmoid', \n        encoder_weights=None, \n        decoder_block_type='transpose'\n    )\n\ndef rle_encode(mask):\n    pixels = mask.flatten(order=\"F\")\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\n# --- 3. INFERENCE LOOP ---\ntest_files = sorted(glob.glob(os.path.join(TEST_IMG_DIR, \"*.png\")))\naccumulated_preds = np.zeros((len(test_files), IMG_H, IMG_W, 1), dtype=np.float16)\n\nprint(f\"ğŸ“ Generatng CSV with Threshold: {NEW_THRESHOLD}\")\nprint(f\"ğŸ“‚ Processing {len(test_files)} images...\")\n\nfor fold in range(1, N_FOLDS + 1):\n    model_path = f'model_fold_{fold}.keras'\n    \n    if not os.path.exists(model_path):\n        print(f\"âš ï¸ Warning: {model_path} not found. Skipping.\")\n        continue\n        \n    print(f\"â¡ï¸ Loading Fold {fold}...\")\n    \n    # Clear Memory\n    tf.keras.backend.clear_session()\n    model = build_model()\n    model.load_weights(model_path)\n    \n    # Predict in batches\n    batch_size = 4\n    for i in tqdm(range(0, len(test_files), batch_size), desc=f\"Fold {fold}\"):\n        batch_files = test_files[i : i + batch_size]\n        batch_imgs = []\n        \n        for path in batch_files:\n            img = cv2.imread(path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (IMG_W, IMG_H))\n            img = preprocess_input(img)\n            batch_imgs.append(img)\n            \n        batch_imgs = np.array(batch_imgs)\n        \n        # 1. Prediction (Standard)\n        p1 = model.predict(batch_imgs, verbose=0)\n        \n        # 2. Prediction (Flip TTA)\n        p2 = model.predict(np.fliplr(batch_imgs), verbose=0)\n        p2 = np.fliplr(p2)\n        \n        # Average\n        batch_pred = (p1 + p2) / 2.0\n        accumulated_preds[i : i + batch_size] += batch_pred.astype(np.float16)\n    \n    del model\n    gc.collect()\n\n# Average across folds\naccumulated_preds /= N_FOLDS\n\n# --- 4. APPLY THRESHOLD & SAVE ---\nprint(f\"ğŸ’¾ Saving to submission.csv with threshold {NEW_THRESHOLD}...\")\nsubmission_rows = []\n\nfor i, path in enumerate(tqdm(test_files, desc=\"Encoding\")):\n    # Resize to original 540x960\n    pred_mask = accumulated_preds[i].astype(np.float32)\n    prob_full = cv2.resize(pred_mask, (960, 540))\n    \n    # ğŸŸ¢ APPLY CUSTOM THRESHOLD HERE ğŸŸ¢\n    mask = (prob_full > NEW_THRESHOLD).astype(np.uint8)\n    \n    # Optional: Fill small holes (Morphology)\n    # kernel = np.ones((5, 5), np.uint8)\n    # mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel) \n    \n    rle = rle_encode(mask)\n    name = os.path.splitext(os.path.basename(path))[0]\n    submission_rows.append({'image_id': name, 'encoded_pixels': rle})\n\ndf_sub = pd.DataFrame(submission_rows)\ndf_sub.to_csv(\"submission.csv\", index=False)\nprint(\"âœ… Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T17:51:40.538465Z","iopub.execute_input":"2026-02-08T17:51:40.538830Z","iopub.status.idle":"2026-02-08T18:09:31.697198Z","shell.execute_reply.started":"2026-02-08T17:51:40.538802Z","shell.execute_reply":"2026-02-08T18:09:31.695815Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ Generatng CSV with Threshold: 0.45\nğŸ“‚ Processing 1002 images...\nâ¡ï¸ Loading Fold 1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fold 1:   0%|          | 0/251 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"059b7b6c51ff44279c9c55ee5b1f940e"}},"metadata":{}},{"name":"stdout","text":"â¡ï¸ Loading Fold 2...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fold 2:   0%|          | 0/251 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14adf0ed603b410085e8f40d4aacae1a"}},"metadata":{}},{"name":"stdout","text":"â¡ï¸ Loading Fold 3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fold 3:   0%|          | 0/251 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e5159e7ceed4509be209c411fbd808a"}},"metadata":{}},{"name":"stdout","text":"â¡ï¸ Loading Fold 4...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fold 4:   0%|          | 0/251 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56946f5b8d534775908564e6df4d0a9a"}},"metadata":{}},{"name":"stdout","text":"â¡ï¸ Loading Fold 5...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fold 5:   0%|          | 0/251 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c264a653d81343ec91c6adedc1e1f95e"}},"metadata":{}},{"name":"stdout","text":"ğŸ’¾ Saving to submission.csv with threshold 0.45...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Encoding:   0%|          | 0/1002 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbacc56c81434beeb74ad0574bab6c89"}},"metadata":{}},{"name":"stdout","text":"âœ… Done!\n","output_type":"stream"}],"execution_count":10}]}